---
title: "ML-enabled systems model deployment and monitoring: Status quo and problems"
collection: publications
category: conferences
permalink: /publication/2024-04-12-ML-EnabledSystemsModelDeploymentMonitoring
date: 2024-04-12
venue: 'International Conference on Software Quality'
paperurl: 'https://link.springer.com/chapter/10.1007/978-3-031-56281-5_7'
citation: 'Zimelewicz, E., Kalinowski, M., Mendez, D., <b>Giray, G.</b>, Santos Alves, A. P., Lavesson, N., ... & Gorschek, T. (2024, April). Ml-enabled systems model deployment and monitoring: Status quo and problems. In <i>International Conference on Software Quality</i> (pp. 112-131). Cham: Springer Nature Switzerland.'
---

<i>Context:</i> Systems that incorporate Machine Learning (ML) models, often referred to as ML-enabled systems, have become commonplace. However, empirical evidence on how ML-enabled systems are engineered in practice is still limited; this is especially true for activities surrounding ML model dissemination. <i>Goal:</i> We investigate contemporary industrial practices and problems related to ML model dissemination, focusing on the model deployment and the monitoring ML life cycle phases. <i>Method:</i> We conducted an international survey to gather practitioner insights on how ML-enabled systems are engineered. We gathered a total of 188 complete responses from 25 countries. We analyze the status quo and problems reported for the model deployment and monitoring phases. We analyzed contemporary practices using bootstrapping with confidence intervals and conducted qualitative analyses on the reported problems applying open and axial coding procedures. <i>Results:</i> Practitioners perceive the model deployment and monitoring phases as relevant and difficult. With respect to model deployment, models are typically deployed as separate services, with limited adoption of MLOps principles. Reported problems include difficulties in designing the architecture of the infrastructure for production deployment and legacy application integration. Concerning model monitoring, many models in production are not monitored. The main monitored aspects are inputs, outputs, and decisions. Reported problems involve the absence of monitoring practices, the need to create custom monitoring tools, and the selection of suitable metrics. <i>Conclusion:</i> Our results help provide a better understanding of the adopted practices and problems in practice and support guiding ML deployment and monitoring research in a problem-driven manner.
